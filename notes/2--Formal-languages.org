#+Title: Formal languages
#+Subtitle: Principles of Programming Languages
#+Author: Mark Armstrong
#+Date: Fall 2020
#+Description: Definition and tools for building formal languages.
#+Description: Introduction to semantics.
#+Options: toc:nil

* HTML settings                                 :noexport:

** Reveal settings

#+Reveal_root: ./reveal.js
#+Reveal_init_options: width:1600, height:900, controlsLayout:'edges',
#+Reveal_init_options: margin: 0.1, minScale:0.125, maxScale:5,
#+Reveal_init_options: mouseWheel: true
#+Reveal_extra_css: local.css

# #+HTML: <script src="https://cdnjs.cloudflare.com/ajax/libs/headjs/0.96/head.min.js"></script>

* LaTeX settings                                :noexport:

#+LaTeX_header: \usepackage{amsthm}
#+LaTeX_header: \theoremstyle{definition}
#+LaTeX_header: \newtheorem{definition}{Definition}[section]

#+LaTeX_header: \usepackage{unicode-math}
#+LaTeX_header: \usepackage{unicode}

* Preamble

** Introduction                                :ignore:

This section introduces the mathematical tools
we will use in the discussion of programming languages
as a /formal/ language.

Several small formal languages (not full programming languages)
are used as examples of the use of these tools.

** TODO Table of contents

# The table of contents are added using org-reveal-manual-toc,
# and so must be updated upon changes or added last.
# Note that hidden headings are included, and so must be deleted!

#+HTML: <font size="-1">
#+begin_scriptsize

#+end_scriptsize
#+HTML: </font>

** TODO Notable references

:TODO:

** Version history

*** September 16th

More complete version posted. Nearly complete up to Parsing.

After lecture, several typos fixed.
First parse tree example also fixed
(the nodes were in the wrong order.)

*** Beginning of course

Very incomplete version of the notes in place.

* Formal languages

** Preamble                                    :ignore:

Recall, from formal language theory:

A language over an /alphabet/ (set of symbols) $Σ$
is a subset of $Σ^{*}$.
The elements of a language are called /sentences/
(or /strings/ or sometimes /words/).

A /formal/ language is one for which we have a mathematical tool
for either
- /generating/ (or /deriving/) all sentences of the language,
  or equivalently,
- /recognising/ (or /accepting/) only sentences of the language.

Examples of such mathematical tools include
- regular expressions,
- automata, and
- grammars.

** The usefulness of formal languages

Formal languages, unlike /natural/ languages, are well-suited
for comprehension by computers.
- Machines require unambiguous steps to follow.
- Hence, all programming languages are formal languages.

In particular, in most cases:
- The sets of keywords, names, etc., form several /regular languages/,
  and so can be recognised by regular expressions.
- The set of valid (in terms of form) programs forms
  a /context-free/ language, and so can be recognised by
  a (context-free) grammar.

** Strings

Recall that given a set $Σ$, the set of strings over $Σ$,
written $Σ^{*}$, is the set of all finite sequences
of elements of $Σ$.

In particular, the sequence of length zero we denote by $ε$.
Note that some other sources use $λ$ for this purpose.

For example, for $Σ = \{a, b, c\}$,
#+begin_center
$Σ^{*} = \{ε, a, b, c, aa, ab, ac, ba, bb, bc, ca, cb, cc, aaa, …\}$.
#+end_center

Given an element $e ∈ Σ$, we write
- $e^{n}$ for the string consisting of $n$ occurrences of $e$, and
- $e^{*}$ for the set $\{ n ∈ ℕ ∣ e^{n} \}$.

* Describing the /syntax/ of formal languages

** Preamble                                    :ignore:

In this section, we will
- briefly review regular expressions and grammars as
  they are presented in formal language theory, and then
- introduce more practical syntax for each
  which is used in practice.

In both cases, the additional syntax only adds to
the /practical expressiveness/ of the tool.
- It does not change the tool's /theoretical expressiveness/.
  - The same set of languages can be described,
    but many languages can be described “more easily”.
- We will present brief arguments to this effect
  by showing how to translate from the new syntax
  to the restricted syntax.

** Regular expressions as in formal language theory

Given a finite alphabet $Σ$,
the set of regular expressions (over $Σ$),
denoted $RE(Σ)$, is given
by the following rules.
1. $∅$, $ε$ and $a$ (for each $a ∈ Σ$) are regular expressions.
2. $(α | β)$, $(αβ)$ and $(α^{*})$ are regular expressions
   - for any regular expressions α and β.

Respectively, the three operations in (2) are called
- “or”,
- “append”, and
- “star” or “repeat”.  

** The language for a regular expression

The language generated/recognised by a regular expression
is defined via a (semantic) function $L : RE(Σ) → Σ^{*}$,
defined as follows.
- $L(∅) = ∅$
- $L(ε) = \{ ε \}$
- $L(a) = \{ a \}$
- $L(α | β) = L(α) ∪ L(β)$
- $L(αβ) = \{ uv | u ∈ L(α) ∧ v ∈ L(b) \}$
- $L(α^*) = (L(α))^*$

** Additional operators for more expressive regular expressions

Regular expressions come up frequently in programming,
and there is a rich set of extensions
to make them easier to construct.

We will not try to extensively list them, but some are listed below,
along with their equivalent “basic” form or,
where that is infeasible to write,
its language.
1. $α^{+} \ \ \ ≈ \ \ \ αα^{*}$
2. $α? \ \ \ ≈ \ \ \ α | ε$
3. $\text{.} \ \ \ ≈ \ \ \ a | b | c | …$ where $Σ = {a, b, c, …}$; i.e., $L(.) = Σ$
4. $[c_{1}…c_{n}] \ \ \ ≈ \ \ \ c_{1} | … | c_{n}$, where each $c_{i}$ is a character.
5. $[\verb!^!c_{1}…c_{n}]$, where $L([\verb!^!c_{1}…c_{n}]) = Σ - [c_{1}…c_{n}]$.
6. $α\{m,n\}$, where $L(α\{m,n\}) = ⋃_{i=m}^{n} L(α)^{i}$
      
** Regular expression examples

:TODO:

** Grammars as in formal language theory

Formally, a context-free grammar is a 4-tuple
#+begin_center
$⟨N, Σ, P, S⟩$
#+end_center
where
- $N$ is a finite set of /non-terminal/ symbols
  (sometimes called variables),
- $Σ$ is the underlying alphabet,
  also called the /terminals/ of the grammar,
- $N$ and $Σ$ must be distinct,
- $P$ is a set of /productions/ i.e.,
  a binary relation between $N$ and $(N ∪ Σ)^{*}$,
  - In other words, a multi-valued function from
    nonterminals to strings of non-terminals and terminals,
- $S$ is a distinguished element of $N$, called the /starting nonterminal/.

** Notations for grammar productions in formal language theory

Given
#+begin_center
$(A, α) ∈ P$,
#+end_center
we write
#+begin_center
$A ⟶ α$
#+end_center
and read it as
#+begin_center
“$A$ produces $α$” or “$A$ expands to $α$”.
#+end_center

Given a number of
productions
#+begin_center
$(A, α₁) ∈ P$, $(A, α₂) ∈ P$, …, $(A, αₘ) ∈ P$,
#+end_center
we write
#+begin_center
$A ⟶ α₁ | α₂ | … | αₘ$
#+end_center
as a shorthand.

** Conventions for grammars

Writing the 4-tuple each time we produce a grammar is tedious.

For this reason, we adopt the following conventions
in order to allow us to omit the 4-tuple.
1. We write /only/ the list of productions.
2. The set $N$ is taken to be the set of all symbols
   appearing to the left of a list of productions.
   - Note that this requires each nonterminal have
     at least one production.
3. The set $Σ$ is usually understood by the context
   in which we are defining the grammer.
   - For our purposes, it will usually be the set of
     all ASCII symbols.
4. The starting nonterminal $S$ is understood to be either
   1. the nonterminal whose name matches that of the grammar
      we are defining (it may be uncapitalised or abbreviated),
   2. otherwise, the non-terminal named $S$, or
   3. otherwise, the nonterminal to the left of
      the first production in the list.
      - (We usually attempt to write grammars “top down”.)
      
** A simple example grammar

#+begin_src text
A ⟶ aAa | B
B ⟶ bBb | C
C ⟶ cCc | ε
#+end_src

This produces the language of strings of
the form
#+begin_center
$a^{i}b^{j}c^{k}c^{k}b^{j}a^{i}$
#+end_center

** Exercise – reading grammars

What languages do the following grammars produce?

#+begin_src text
A ⟶ B | C
B ⟶ aaB | ε
C ⟶ aaaC | ε
#+end_src

#+begin_src text
A ⟶ aB | B | ε
B ⟶ bC | C
C ⟶ cA | A
#+end_src

#+begin_src text
A ⟶ aA | B
B ⟶ bB
#+end_src

*What's the tricky part with the last one?*

Extra exercise: can you simplify any of them?
For instance, by having less non-terminals or less productions?
If you believe so, just be careful that
your simplification accepts the same string!

** Grammars generate or recognise strings

We have discussed the facts that a grammar can
- generate strings or
- recognise/accept strings.

Then for a grammar $G$ we might think of functions
- $generateᴳ : ℕ → Σ^{*}$
  - with the intention that $generateᴳ(n)$ generates the $n^{th}$
    string in the grammar's language is lexicographic order
- $recogniseᴳ : Σ^{*} → Bool$
That is, we have two functions, which output a ~String~ or
a ~Bool~ respectively.

But there is a useful byproduct which may be obtained during
during either process: a /parse tree/.

** Parse trees

A parse tree's
- nodes (which have children) are
  labelled by a nonterminal of the grammar,
- leaves (which do not have children) are
  labelled by a terminal of the grammar, and
- if a node is labelled by a nonterminal ~A~,
  the children of that node must correspond to
  (in order from left to right)
  the terminals and nonterminals appearing in a production of ~A~.
  If a non-terminal would produce ~ε~, it is omitted.

** Example parse tree

For example, consider the grammar
#+begin_src text
S ⟶ AB
A ⟶ aA | ε
B ⟶ Bb | b
#+end_src

We have the following parse tree for the string ~aab~.
- Note the dashed portions, which show part of how the tree
  was derived from the grammar,
  but which will usually be omitted by our rules for parse trees.
#+begin_src dot :file media/parse-tree-example-aab.png
digraph T {
  S  [label="S"]
  A1 [label="A"]
  A2 [label="A"]
  A3 [label="A", style=dashed]
  B  [label="B"]

  a1 [label="a", shape=plaintext]
  a2 [label="a", shape=plaintext]
  b  [label="b", shape=plaintext]
  eps [label="ε", style=dashed]
  
  S -> A1 -> a1
  { rank=same; a1 -> A2 [style=invis] }
       A1 -> A2 -> a2
  { rank=same; a2 -> A3 [style=invis] }
       A2 -> A3 [style=dashed]
       A3 -> eps [style=dashed]
  
  S -> B  -> b
}
#+end_src

#+RESULTS:
[[file:media/parse-tree-example-aab.png]]

** Another example parse tree

Similarly, working with the same grammar,
we have the following parse tree for ~abb~.
#+begin_src dot :file media/parse-tree-example-abb.png
digraph T {
  S  [label="S"]
  A  [label="A"]
  B1 [label="B"]
  B2 [label="B"]

  a  [label="a", shape=plaintext]
  b1 [label="b", shape=plaintext]
  b2 [label="b", shape=plaintext]

  S -> A  -> a
  S -> B1 -> b1
       B1 -> B2 -> b2
}
#+end_src

#+RESULTS:
[[file:media/parse-tree-example-abb.png]]

** Exercise: creating parse trees

Exercise: provide a parse tree for the string ~aaa~ using this grammar.
Is there a valid parse tree for the string ~bbb~?

Exercise: if we add a production ~A ⟶ a~ to our example grammar,
can you provide a different parse tree
(or multiple different parse trees) for ~aaa~?

** Backus-Naur form (BNF)

Up until now, we have used the form
#+begin_example text
N₁ ⟶ P₁ | P₂ | …
   ⋮
#+end_example
for our production lists.

Commonly in the study of programming languages,
an alternative syntax called /Backus-Naur/ form (BNF)
is used.
- Named for two members of the ALGOL design committee,
  who created the first formal definition for a programming language,
  namely ALGOL.

** BNF details

In Backus-Naur form,
- all nonterminals names are delimited by
  angle brackets, ~⟨⟩~,
  - (if using ASCII characters, ~<>~)
- the ~⟶~ is replaced by ~∷=~,
- additional whitespace is permitted on the right side
  of a production between terminals and nonterminals,
  without changing the meaning of the production
  - So $⟨A⟩ ∷= a\ a\ ⟨A⟩$ is treated the same as $⟨A⟩ ∷= aa⟨A⟩$.

** Aside: ALGOL

ALGOL (for “ALGOrithmic Language”)
was a contemporary of Fortran, Lisp, and Cobol.
- Together, those three are the oldest languages
  still in (fairly) common use today.
  - Granted, not the same versions.

Specifically, there were several iterations of ALGOL,
the three major ones being ALGOL 58, ALGOL 60 and ALGOL 68.

ALGOL is not in common use, but it was
the most influential on modern programming language syntax,
introducing concepts such as the block.
- The “C family” can trace its lineage directly to ALGOL.

** Extended Backus-Naur form (EBNF)

We further extend our grammar notation to include several
several additional operators.
- These extensions are part of the /extended/ Backus-Naur form.
- Once again, this is only an extension in the /practicality/ sense.

There is an [[https://www.iso.org/standard/26153.html][ISO standard]] for EBNF.
Our syntax and inclusion of features is
not chosen to match the standard;
it is what is convenient for our use.

** EBNF details

- (Square) brackets, ~[]~, surrounding a string
  indicate that string may or may not be included in a production.
  - I.e., they make part of a production optional.
  - $⟨A⟩ ∷= α₁ [ α₂ ] α₃ \ \ \ ≈ \ \ \ ⟨A⟩ ∷= α₁ α₂ α₃ | α₁ α₃$.
- (Curly) braces, ~{}~, surrounding a string
  indicate that string may be repeated any number of times,
  including zero.
  - $⟨A⟩ ∷= α₁ { α₂ } α₃ \ \ \ ≈ \ \ \ ⟨A⟩ ∷= α₁ ⟨A′⟩ α₃$, $⟨A′⟩ ∷= α₂ ⟨A′⟩ | ε$.
- Parentheses, ~()~, may group parts of a string.
- The “alternative” pipe, ~|~, may be used /inside/ of productions,
  to indicate alternatives inside a set of brackets, braces
  or parentheses.
  - $⟨A⟩ ∷= α₁ (α₂ | α₃) α₄ \ \ \ ≈ \ \ \ ⟨A⟩ ∷= α₁ α₂ α₄ | α₁ α₃ α₄$.
- Where necessary, terminals may be single or double quoted,
  such as to indicate a whitespace character, pipe or quote.
  - $⟨\text{ebnfprods}⟩ ∷= ⟨\text{string}⟩ | ⟨\text{string}⟩ ⟨\text{optws}⟩ “|” ⟨\text{optws}⟩ ⟨\text{ebnfprods}⟩$

** Exercise – translating to EBNF

Translate this grammar from an earlier exercise to EBNF syntax.
#+begin_src text
A ⟶ B | C
B ⟶ aaB | ε
C ⟶ aaaC | ε
#+end_src
Then try to reduce the number of productions in the grammar,
while maintaining the language defined.

Can you use only one production when using EBNF?

** EBNF's syntactic sugar

EBNF gives us our first example of /syntactic sugar/;
syntax that does not add new features to a language,
only more convenient notation.
- As shown above, any grammar using the additional operators
  can be translated into one not using them.
  - But this likely requires more productions.
  - And certainly more characters/space on the page.
  
Syntactic sugar is a common feature of programming languages.
- Example: (imperative) languages often include various kinds of loops,
  where only one (or sometimes none!) is truly necessary.

When we discuss programming languages formally,
we will usually omit constructs which are syntactic sugar.
- If anything, we may note how to represent them
  in a “core” language which includes less constructs.

** Exercise – a small language C-like language

Consider the following context-free language.
#+begin_example text
⟨stmt⟩   ∷= ⟨assign⟩ | ⟨stmt⟩ "; " ⟨stmt⟩ | "while " ⟨expr⟩ " do " ⟨stmt⟩ | ⟨ws⟩ ⟨stmt⟩ ⟨ws⟩
⟨assign⟩ ∷= ⟨var⟩ ⟨ws⟩ " := " ⟨expr⟩
⟨expr⟩   ∷= ⟨var⟩ | ⟨const⟩ | ⟨expr⟩ ⟨op⟩ ⟨expr⟩ | ⟨ws⟩ ⟨expr⟩ ⟨ws⟩
⟨var⟩    ∷= ('x' | 'y' | 'z') {⟨var⟩}
⟨const⟩  ∷= (1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0) {⟨const⟩}
⟨op⟩     ∷= '+' | '-' | '*' | '/' | '<' | '>' | '='
⟨ws⟩     ∷= {' '} | {'\n'}
#+end_example

Provide some example programs in this language.

Can you precisely describe the language in English?

** Example – EBNF for C++

A good example of the practicality EBNF for specifying
the syntax of languages is this
[[http://www.externsoft.ch/download/cpp-iso.html][EBNF grammar for C++]]
(presented in tabular form, rather than lists of productions
as we use).

The grammar is much, much larger than anything we will write,
but it is still quite concise for describing
a real-world programming language.

* Parsing and executable code

** Preamble                                    :ignore:

We will briefly summarise the parsing process,
beginning with some important terms.
- In this course, we are primarily interested in
  the beginning of this process, up to the
  construction of parse trees.

** Atomic syntactic units

We have mentioned that both regular expressions and
context-free grammars are used in the description of
the syntax of programming languages.

However, our example programming language earlier
was described exclusively by a context-free grammar.
- Even the smallest syntactic units of the language,
  the /atomic/ syntactic units, have been described by the grammars.
  - For instance, we have used the production
    $⟨const⟩  ∷= (1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0) \{⟨const⟩\}$
    which describes numerical constants.

This is not done in practice.

** Lexemes and tokens

In practice,
- regular expressions are instead used to describe the
  atomic syntactic units of languages.
  - For example,
    - keywords such as ~if~ and ~while~, constant values such as ~0~ or ~"abc"~,
      or names such as ~height~ or ~sqrt~.
  - Lexemes cannot be broken down into meaningful pieces.
- Grammars are then used to describe the possible arrangements
  of lexemes.
  - The terminals of the grammar are then names for sets of lexemes,
    called /tokens/, rather than elements of $Σ$.
  - For instance,
    - the token ~while~ for the set containing only the
      keyword ~while~,
    - or the token ~int_literal~ for the set $\{ 0, 1, -1, 2, … \}$,
    - or the token ~var~ for the set of valid variable names.

** Parsing

Parsing is the process of translating a program
from plaintext
to executable instructions
- whether this is done
  - ahead of time (compiling) or
  - when the program is to be run (interpreting),
  parsing is a necessary step before execution.
- A computer cannot run unparsed higher level language code.
  
** The zeroth step – preprocessing

Many programming languages support some form
of /preprocessing directives/ which are
to be carried out before the parsing process
properly begins.
- Commonly, “macros”, which often are simply
  textual substitutions to be carried out.
  - But they can be used for significantly more;
    in some instances, macros form a whole language themselves.

** The first step – lexical analysis

:TODO: There's a break in the narrative here.

We now know the first step in parsing.
- Convert the plaintext source code into a sequence of tokens.
  - This process may be
    called /lexical analysis/, /lexing/ or /tokenising/.
  - The program to carry this process out may be
    called a /lexer/ or /tokeniser/.
  - Lexical analysis discards whitespace, comments, and any other
    irrelevant text.

** The second step – parsing (syntactic analysis)

After converting from plaintext to a string of tokens, the next
step of parsing is to construct the parse tree.

This step is part of the parsing process,
but it is also usually called parsing.
- It may also be called /syntactic analysis/.

** The third step – (static) semantic analysis

:TODO:

** The fourth step – intermediate code generation

:TODO:

# Mention optimisation

** Visualising the entire parsing process

#+begin_src ditaa :file media/parsing-whole.png :exports results
+-----------+    /--------------\    +--------------------+
| {d}       |    |              |    | {d}                |
| Plaintext +----+ Preprocesser +--->+ Expanded plaintext +-\
| cDDF      |    | cFDD         |    | cDFD               | |
+-----+-----+    \--------------/    +--------------------+ |
                                                            |
             /----------------------------------------------/
             |
/------------+---------------\    +--------------------------+
|                            |    | {d}                      |
|     Lexical analysis       |    |    Sequence of tokens    |
| (constructed from regexps) +--->| (tagged with attributes) +-\
| cFDD                       |    | cDFD                     | |
\----------------------------/    +-----------+--------------+ |
                                                               |
             /-------------------------------------------------/
             |
/------------+---------------\    +----------------------+
|                            |    | {d}                  |
|          Parser            |    |                      |
| (constructed from grammar) +--->|  +-\
| cFDD                       |    | cDFD                 | |
\----------------------------/    +----------------------+ |
                                                           |
              /--------------------------------------------/
              |
/-------------+--------------\    +-------------------------+
|                            |    | {d}                     |
| (Static) semantic analysis |    | Attributed syntax tree, |
|                            +--->|    symbol table         +-\
| cFDD                       |    | cDFD                    | |
\----------------------------/    +-------------------------+ |
                                                              |
              /-----------------------------------------------/
              |
/-------------+---------------\    +-----------------------+
|                             |    | {d}                   |
| Intermediate code generator |    | Intermediate language |
|                             +--->+          code         +-\
|  cFDD                       |    | cDFD                  | |
\-----------------------------/    +-----------------------+ |
                                                             |
         /---------------------------------------------------/
         |
/--------+--------------\    +-----------------+
|                       |    | {d}             |
| Intermediate language |    | Executable code |
|     implemention      |--->|                 |
| cFDD                  |    | cDDF            |
\-----------------------/    +-----------------+
#+end_src

#+RESULTS:
[[file:media/parsing-whole.png]]

* Ambiguity

** Ambiguity

Recall that parsing a string (or deriving a string)
using a grammar gives rise to a /parse tree/ or /derivation tree/.

In many cases, there is more than one parse tree
for a given string in the language produced by a grammar.

For instance, the string ~aa~ has four valid parse trees
under the grammar
#+begin_src text
⟨A⟩ ∷= a ⟨A⟩ | ⟨A⟩ a | ε 
#+end_src

Exercise: find all four valid parse trees for ~aa~ with the above
grammar.

** Removing ambiguity

It is desirable to have a single parse tree for every program.
- We should not admit two syntactic interpretations for a program!

Three tools for removing ambiguity are
- requiring parentheses,
- introducing precedence rules, and
- introducing associativity rules.

** Enforcing precedence and associativity with grammars

To enforce precedence using a grammar:
- Create a hierarchy of non-terminals.
- Higher-precedence operators are produced lower in the hierarchy.
- For instance,
  - An additive term can be a addition of multiplicative terms,
    which is an addition of literals, which can be the negation
    of a constant, variable or term.

To enforce associativity using a grammar:
- Left associative operators should be produced by left recursive
  non-terminals.
- And right associative operators by right recursive non-terminals.
- Operators of the same precedence must associate the same way!

** Is addition associative?

Recall that addition is an associative operator.
- Meaning it is both left and right associative.

So the choice of whether addition in a language associates to
the right or to the left may seem arbitrary.
- But numerical types in programming are not necessarily
  the same as numerical types in math!
- Addition of floating point numbers /is not associative/.
  - Consider a binary representation with two-digit coefficients.
  - 1.0₂ × 2⁰ + 1.0₂ × 2⁰ + 1.0₂ × 2² has a different value depending
    upon parenthesisation.

** Abstract syntax

“Simple”, ambiguous grammars do have a place in describing
programming language syntax.
- Such grammars describe the /abstract syntax/ of the language.
  - As opposed to /concrete syntax/.
- Consider programs as /trees/ generated by the grammar
  for the abstract syntax of the language.
  - Trees do not admit ambiguity!
  - Such trees more efficiently represent programs.
    - The shape of the tree expresses structure.
    - Other unnecessary details may be left out.

** Beyond context-free grammars: “static semantics”

For most interesting languages,
context-free grammars are not quite sufficient
to describe well-formed programs.
- They cannot express conditions such as
  “variables must be declared before use”, and
  typing rules.
- It has been /proven/ that CFGs are not sufficient.
  - At least some typing rules are possible to express,
    but prohibitively difficult.

Recall the Chomsky hierarchy of languages.
#+begin_src text
Regular ⊂ Context-free ⊂ Context-sensitive ⊂ Recursive ⊂ Recursively enumberable
#+end_src
- The properties we need could be described by /context-sensitive/ grammars.
  - But they are unwieldy!
- Instead, use /attribute grammars/;
  a relatively small augmentation to CFGs.
  - Each non-terminal and terminal may have a collection
    of /attributes/ (named values).
  - Each production may have a collection of
    rules defining the values of the attributes
    and a collection of predicates
    reasoning about those attributes.

** An example attribute grammar

Consider this simple grammar.
#+begin_src text
⟨S⟩ ∷= ⟨A⟩ ⟨B⟩ ⟨C⟩
⟨A⟩ ∷= ε ∣ a ⟨A⟩
⟨B⟩ ∷= ε ∣ b ⟨B⟩
⟨C⟩ ∷= ε ∣ c ⟨C⟩
#+end_src

Suppose we want to allow only strings of the form ~aⁿbⁿcⁿ~.
There is no CFG that can produce exactly such strings.
But we can enforce this condition using the above grammar
augmented with attributes.
- Each of the non-terminals ~⟨A⟩~, ~⟨B⟩~ and ~⟨C⟩~ are given an attribute
  ~length~.
- To each production with ~⟨A⟩~, ~⟨B⟩~ or ~⟨C⟩~ on the left side, we attach
  a rule to compute the ~length~.
- The production ~⟨S⟩ ∷= ⟨A⟩ ⟨B⟩ ⟨C⟩~ enforces the condition with a predicate.

#+REVEAL: split:t

#+begin_src text
⟨S⟩ ∷= ⟨A⟩ ⟨B⟩ ⟨C⟩
Predicate: ⟨A⟩.length = ⟨B⟩.length = ⟨C⟩.length

⟨A⟩ ∷= ε
Rule: ⟨A⟩.length ≔ 0

⟨A⟩₁ ∷= a ⟨A⟩₂
Rule: ⟨A⟩₁.length ≔ ⟨A⟩₂.length + 1

⟨B⟩ ∷= ε
Rule: ⟨B⟩.length ≔ 0

⟨B⟩₁ ∷= b ⟨B⟩₂
Rule: ⟨B⟩₁.length ≔ ⟨B⟩₂.length + 1

⟨C⟩ ∷= ε
Rule: ⟨C⟩.length ≔ 0

⟨C⟩₁ ∷= c ⟨C⟩₂
Rule: ⟨C⟩₁.length ≔ ⟨C⟩₂.length + 1
#+end_src

In productions with multiple occurrences of the same non-terminal,
we number the occurrences so we can easily refer to them
in the rules/predicates.

* Abstract and concrete syntax; ignoring ambiguity
* The /semantics/ of formal languages

** COMMENT Old notes

Unlike with syntax, there is not one universally used tool
for describing programming language semantics.

In this course we will primarily consider /operational semantics/.
- A formal description of the meaning programs as
  a series of computation steps on an abstract machine.
  - The machine should be more abstract, and more easily understood,
    than assembly language.
  - But still “simpler” than the language.
  - Stack machines and state diagrams are good candidates.

Additional approaches include
- Denotational semantics.
  - The meaning of programs are /denoted/ by mathematical objects.
    - Such as partial functions.
  - Have to consider /limits/ and non-termination.
- Axiomatic semantics.
  - The meaning of a program is given by a precondition/postcondition
    calculus.
    - Such as ~wp~; the “weakest-precondition” calculus.
  - Very useful for specification.

*** The kernel language approach

The “kernel language” approach to semantics can be used
for languages with many features and constructs.
- Choose a small “kernel” set of features/constructs.
- Describe the remainder of the language in terms of that kernal language.
- The kernel language may be described using the formal approaches
  mentioned.
- /Concepts, Techniques, and Models of Computer Programming/
  takes this approach.

*** More to come...

We will return to the discussion of semantics later in the course.

** Preamble                                    :ignore:

The /semantics/ of a language assigns a meaning to each sentence.
- In order to define a semantics, we must
  have in mind a /semantic domain/;
  - a domain of meanings into which we map sentences.
- For instance, if we are defining a language
  of natural numbers /Nat/, we will map sentences into the set ~ℕ~.
- Or map elements of a languages of propositions into ~𝔹~.
- We may often provide several different definitions of
  a particular mapping, to emphasise different details.

We may also have several semantic domains for a given language.
- In the case of programming languages,
  several domains of meaning have been proposed and used;
  the three most well known are
  - computing devices, whether a real-world machine or an /abstract/ machine,
    - this is known as /operational semantics/
  - (mathematical) functions,
    - this is known as /denotational semantics/
  - precondition/postcondition pairs
    - this is known as /axiomatic semantics/

** Example – semantics of a language of natural numbers

Consider again a language of terms intended to represent
natural numbers.
#+begin_src text
⟨nat⟩ ∷= "zero" | "suc" ⟨nat⟩ 
#+end_src

To assign meaning to these terms,
we introduce a mapping from these (concrete) terms
to (abstract) numerals.
#+begin_src text
eval zero = 0
eval (suc n) = (eval n) + 1
#+end_src

The evaluation function in this case is very obvious and trivial,
because with this language is simply a concrete representation
of the semantic domain.
- In comparison, when defining the semantics of programming languages,
  the language and the semantic domain are not so directly related.

** Example – semantics of propositional logic

As a more complex example, we can map propositional logic terms
into the set of booleans.
#+begin_src text
⟨prop⟩ ∷= "tt" | "ff" | ¬ ⟨prop⟩ | ⟨prop⟩ (∧ | ∨ | ⇒ | ⇔) ⟨prop⟩
#+end_src

In order to make the mapping less trivial, let us define it
without using boolean combinators; only constants
and “if-then-else” statements.
#+begin_src text
eval tt = true
eval ff = false

eval (¬ p) = true    if eval p
             false   otherwise

eval (p ∧ q) = eval q   if eval p
               false    otherwise

…
#+end_src
Exercise: Complete this evaluation function.

** Example – small-step semantics of propositional logic

The evaluation function defined above can be considered
to be a /big-step/ semantics.
- It is a (single-valued) relation between terms and
  their (final) value.

In contrast, we may define a /small-step/ semantics
- which maps terms to terms which are “one step” simpler.
- Then, once we have reduced to a constant term, that may be mapped
  to a value (this part is not shown here).
#+begin_src text
reduce (¬ tt) = ff
reduce (¬ ff) = tt
reduce (¬ p)  = ¬ (reduce p)

reduce (tt ∧ q) = reduce q
reduce (ff ∧ q) = ff
reduce (p ∧ q)  = (reduce p) ∧ q

…
#+end_src
Exercise: Complete this reduction function.
