#+Title: Formal languages
#+Subtitle: Principles of Programming Languages
#+Author: Mark Armstrong
#+Date: Fall 2020
#+Description: Definition and tools for building formal languages.
#+Description: Introduction to semantics.
#+Options: toc:nil

* HTML settings                                 :noexport:

** Reveal settings

#+Reveal_root: ./reveal.js
#+Reveal_init_options: width:1600, height:900, controlsLayout:'edges',
#+Reveal_init_options: margin: 0.1, minScale:0.125, maxScale:5,
#+Reveal_init_options: mouseWheel: true
#+Reveal_extra_css: local.css

# #+HTML: <script src="https://cdnjs.cloudflare.com/ajax/libs/headjs/0.96/head.min.js"></script>

* LaTeX settings                                :noexport:

#+LaTeX_header: \usepackage{amsthm}
#+LaTeX_header: \theoremstyle{definition}
#+LaTeX_header: \newtheorem{definition}{Definition}[section]

#+LaTeX_header: \usepackage{unicode-math}
#+LaTeX_header: \usepackage{unicode}

* Preamble
:PROPERTIES:
:CUSTOM_ID: Preamble
:END:

** Introduction                                :ignore:

This section introduces the mathematical tools
we will use in the discussion of programming languages
as a /formal/ language.

Several small formal languages (not full programming languages)
are used as examples of the use of these tools.

** TODO Table of contents
:PROPERTIES:
:CUSTOM_ID: Table-of-contents
:END:

# The table of contents are added using org-reveal-manual-toc,
# and so must be updated upon changes or added last.
# Note that hidden headings are included, and so must be deleted!

#+HTML: <font size="-1">
#+begin_scriptsize

#+end_scriptsize
#+HTML: </font>

** TODO Notable references
:PROPERTIES:
:CUSTOM_ID: Notable-references
:END:

:TODO:

** Version history
:PROPERTIES:
:CUSTOM_ID: Version-history
:END:

*** September 16th
:PROPERTIES:
:CUSTOM_ID: September-16th
:END:

More complete version posted. Nearly complete up to Parsing.

After lecture, several typos fixed.
First parse tree example also fixed
(the nodes were in the wrong order.)

*** Beginning of course
:PROPERTIES:
:CUSTOM_ID: Beginning-of-course
:END:

Very incomplete version of the notes in place.

* Formal languages
:PROPERTIES:
:CUSTOM_ID: Formal-languages
:END:

** Preamble                                    :ignore:

Recall, from formal language theory:

A language over an /alphabet/ (set of symbols) $Σ$
is a subset of $Σ^{*}$.
The elements of a language are called /sentences/
(or /strings/ or sometimes /words/).

A /formal/ language is one for which we have a mathematical tool
for either
- /generating/ (or /deriving/) all sentences of the language,
  or equivalently,
- /recognising/ (or /accepting/) only sentences of the language.

Examples of such mathematical tools include
- regular expressions,
- automata, and
- grammars.

** The usefulness of formal languages
:PROPERTIES:
:CUSTOM_ID: The-usefulness-of-formal-languages
:END:

Formal languages, unlike /natural/ languages, are well-suited
for comprehension by computers.
- Machines require unambiguous steps to follow.
- Hence, all programming languages are formal languages.

In particular, in most cases:
- The sets of keywords, names, etc., form several /regular languages/,
  and so can be recognised by regular expressions.
- The set of valid (in terms of form) programs forms
  a /context-free/ language, and so can be recognised by
  a (context-free) grammar.

** Strings
:PROPERTIES:
:CUSTOM_ID: Strings
:END:

Recall that given a set $Σ$, the set of strings over $Σ$,
written $Σ^{*}$, is the set of all finite sequences
of elements of $Σ$.

In particular, the sequence of length zero we denote by $ε$.
Note that some other sources use $λ$ for this purpose.

For example, for $Σ = \{a, b, c\}$,
#+begin_center
$Σ^{*} = \{ε, a, b, c, aa, ab, ac, ba, bb, bc, ca, cb, cc, aaa, …\}$.
#+end_center

Given an element $e ∈ Σ$, we write
- $e^{n}$ for the string consisting of $n$ occurrences of $e$, and
- $e^{*}$ for the set $\{ n ∈ ℕ ∣ e^{n} \}$.

* Describing the /syntax/ of formal languages
:PROPERTIES:
:CUSTOM_ID: Describing-the-/syntax/-of-formal-languages
:END:

** Preamble                                    :ignore:

In this section, we will
- briefly review regular expressions and grammars as
  they are presented in formal language theory, and then
- introduce more practical syntax for each
  which is used in practice.

In both cases, the additional syntax only adds to
the /practical expressiveness/ of the tool.
- It does not change the tool's /theoretical expressiveness/.
  - The same set of languages can be described,
    but many languages can be described “more easily”.
- We will present brief arguments to this effect
  by showing how to translate from the new syntax
  to the restricted syntax.

** Regular expressions as in formal language theory
:PROPERTIES:
:CUSTOM_ID: Regular-expressions-as-in-formal-language-theory
:END:

Given a finite alphabet $Σ$,
the set of regular expressions (over $Σ$),
denoted $RE(Σ)$, is given
by the following rules.
1. $∅$, $ε$ and $a$ (for each $a ∈ Σ$) are regular expressions.
2. $(α | β)$, $(αβ)$ and $(α^{*})$ are regular expressions
   - for any regular expressions α and β.

Respectively, the three operations in (2) are called
- “or”,
- “append”, and
- “star” or “repeat”.  

** The language for a regular expression
:PROPERTIES:
:CUSTOM_ID: The-language-for-a-regular-expression
:END:

The language generated/recognised by a regular expression
is defined via a (semantic) function $L : RE(Σ) → Σ^{*}$,
defined as follows.
- $L(∅) = ∅$
- $L(ε) = \{ ε \}$
- $L(a) = \{ a \}$
- $L(α | β) = L(α) ∪ L(β)$
- $L(αβ) = \{ uv | u ∈ L(α) ∧ v ∈ L(b) \}$
- $L(α^*) = (L(α))^*$

** Additional operators for more expressive regular expressions
:PROPERTIES:
:CUSTOM_ID: Additional-operators-for-more-expressive-regular-expressions
:END:

Regular expressions come up frequently in programming,
and there is a rich set of extensions
to make them easier to construct.

We will not try to extensively list them, but some are listed below,
along with their equivalent “basic” form or,
where that is infeasible to write,
its language.
1. $α^{+} \ \ \ ≈ \ \ \ αα^{*}$
2. $α? \ \ \ ≈ \ \ \ α | ε$
3. $\text{.} \ \ \ ≈ \ \ \ a | b | c | …$ where $Σ = {a, b, c, …}$; i.e., $L(.) = Σ$
4. $[c_{1}…c_{n}] \ \ \ ≈ \ \ \ c_{1} | … | c_{n}$, where each $c_{i}$ is a character.
5. $[\verb!^!c_{1}…c_{n}]$, where $L([\verb!^!c_{1}…c_{n}]) = Σ - [c_{1}…c_{n}]$.
6. $α\{m,n\}$, where $L(α\{m,n\}) = ⋃_{i=m}^{n} L(α)^{i}$
      
** Regular expression examples
:PROPERTIES:
:CUSTOM_ID: Regular-expression-examples
:END:

:TODO:

** Grammars as in formal language theory
:PROPERTIES:
:CUSTOM_ID: Grammars-as-in-formal-language-theory
:END:

Formally, a context-free grammar is a 4-tuple
#+begin_center
$⟨N, Σ, P, S⟩$
#+end_center
where
- $N$ is a finite set of /non-terminal/ symbols
  (sometimes called variables),
- $Σ$ is the underlying alphabet,
  also called the /terminals/ of the grammar,
- $N$ and $Σ$ must be distinct,
- $P$ is a set of /productions/ i.e.,
  a binary relation between $N$ and $(N ∪ Σ)^{*}$,
  - In other words, a multi-valued function from
    nonterminals to strings of non-terminals and terminals,
- $S$ is a distinguished element of $N$, called the /starting nonterminal/.

** Notations for grammar productions in formal language theory
:PROPERTIES:
:CUSTOM_ID: Notations-for-grammar-productions-in-formal-language-theory
:END:

Given
#+begin_center
$(A, α) ∈ P$,
#+end_center
we write
#+begin_center
$A ⟶ α$
#+end_center
and read it as
#+begin_center
“$A$ produces $α$” or “$A$ expands to $α$”.
#+end_center

Given a number of
productions
#+begin_center
$(A, α₁) ∈ P$, $(A, α₂) ∈ P$, …, $(A, αₘ) ∈ P$,
#+end_center
we write
#+begin_center
$A ⟶ α₁ | α₂ | … | αₘ$
#+end_center
as a shorthand.

** Conventions for grammars
:PROPERTIES:
:CUSTOM_ID: Conventions-for-grammars
:END:

Writing the 4-tuple each time we produce a grammar is tedious.

For this reason, we adopt the following conventions
in order to allow us to omit the 4-tuple.
1. We write /only/ the list of productions.
2. The set $N$ is taken to be the set of all symbols
   appearing to the left of a list of productions.
   - Note that this requires each nonterminal have
     at least one production.
3. The set $Σ$ is usually understood by the context
   in which we are defining the grammer.
   - For our purposes, it will usually be the set of
     all ASCII symbols.
4. The starting nonterminal $S$ is understood to be either
   1. the nonterminal whose name matches that of the grammar
      we are defining (it may be uncapitalised or abbreviated),
   2. otherwise, the non-terminal named $S$, or
   3. otherwise, the nonterminal to the left of
      the first production in the list.
      - (We usually attempt to write grammars “top down”.)
      
** A simple example grammar
:PROPERTIES:
:CUSTOM_ID: A-simple-example-grammar
:END:

#+begin_src text
A ⟶ aAa | B
B ⟶ bBb | C
C ⟶ cCc | ε
#+end_src

This produces the language of strings of
the form
#+begin_center
$a^{i}b^{j}c^{k}c^{k}b^{j}a^{i}$
#+end_center

** Exercise – reading grammars
:PROPERTIES:
:CUSTOM_ID: Exercise-–-reading-grammars
:END:

What languages do the following grammars produce?

#+begin_src text
A ⟶ B | C
B ⟶ aaB | ε
C ⟶ aaaC | ε
#+end_src

#+begin_src text
A ⟶ aB | B | ε
B ⟶ bC | C
C ⟶ cA | A
#+end_src

#+begin_src text
A ⟶ aA | B
B ⟶ bB
#+end_src

*What's the tricky part with the last one?*

Extra exercise: can you simplify any of them?
For instance, by having less non-terminals or less productions?
If you believe so, just be careful that
your simplification accepts the same string!

** Grammars generate or recognise strings
:PROPERTIES:
:CUSTOM_ID: Grammars-generate-or-recognise-strings
:END:

We have discussed the facts that a grammar can
- generate strings or
- recognise/accept strings.

Then for a grammar $G$ we might think of functions
- $generateᴳ : ℕ → Σ^{*}$
  - with the intention that $generateᴳ(n)$ generates the $n^{th}$
    string in the grammar's language is lexicographic order
- $recogniseᴳ : Σ^{*} → Bool$
That is, we have two functions, which output a ~String~ or
a ~Bool~ respectively.

But there is a useful byproduct which may be obtained during
during either process: a /parse tree/.

** Parse trees
:PROPERTIES:
:CUSTOM_ID: Parse-trees
:END:

A parse tree's
- nodes (which have children) are
  labelled by a nonterminal of the grammar,
- leaves (which do not have children) are
  labelled by a terminal of the grammar, and
- if a node is labelled by a nonterminal ~A~,
  the children of that node must correspond to
  (in order from left to right)
  the terminals and nonterminals appearing in a production of ~A~.
  If a non-terminal would produce ~ε~, it is omitted.

** Example parse tree
:PROPERTIES:
:CUSTOM_ID: Example-parse-tree
:END:

For example, consider the grammar
#+begin_src text
S ⟶ AB
A ⟶ aA | ε
B ⟶ Bb | b
#+end_src

We have the following parse tree for the string ~aab~.
- Note the dashed portions, which show part of how the tree
  was derived from the grammar,
  but which will usually be omitted by our rules for parse trees.
#+begin_src dot :file media/parse-tree-example-aab.png
digraph T {
  S  [label="S"]
  A1 [label="A"]
  A2 [label="A"]
  A3 [label="A", style=dashed]
  B  [label="B"]

  a1 [label="a", shape=plaintext]
  a2 [label="a", shape=plaintext]
  b  [label="b", shape=plaintext]
  eps [label="ε", style=dashed]
  
  S -> A1 -> a1
  { rank=same; a1 -> A2 [style=invis] }
       A1 -> A2 -> a2
  { rank=same; a2 -> A3 [style=invis] }
       A2 -> A3 [style=dashed]
       A3 -> eps [style=dashed]
  
  S -> B  -> b
}
#+end_src

#+RESULTS:
[[file:media/parse-tree-example-aab.png]]

** Another example parse tree
:PROPERTIES:
:CUSTOM_ID: Another-example-parse-tree
:END:

Similarly, working with the same grammar,
we have the following parse tree for ~abb~.
#+begin_src dot :file media/parse-tree-example-abb.png
digraph T {
  S  [label="S"]
  A  [label="A"]
  B1 [label="B"]
  B2 [label="B"]

  a  [label="a", shape=plaintext]
  b1 [label="b", shape=plaintext]
  b2 [label="b", shape=plaintext]

  S -> A  -> a
  S -> B1 -> b1
       B1 -> B2 -> b2
}
#+end_src

#+RESULTS:
[[file:media/parse-tree-example-abb.png]]

** Exercise: creating parse trees
:PROPERTIES:
:CUSTOM_ID: Exercise:-creating-parse-trees
:END:

Exercise: provide a parse tree for the string ~aaa~ using this grammar.
Is there a valid parse tree for the string ~bbb~?

Exercise: if we add a production ~A ⟶ a~ to our example grammar,
can you provide a different parse tree
(or multiple different parse trees) for ~aaa~?

** Backus-Naur form (BNF)
:PROPERTIES:
:CUSTOM_ID: Backus-Naur-form-(BNF)
:END:

Up until now, we have used the form
#+begin_example text
N₁ ⟶ P₁ | P₂ | …
   ⋮
#+end_example
for our production lists.

Commonly in the study of programming languages,
an alternative syntax called /Backus-Naur/ form (BNF)
is used.
- Named for two members of the ALGOL design committee,
  who created the first formal definition for a programming language,
  namely ALGOL.

** BNF details
:PROPERTIES:
:CUSTOM_ID: BNF-details
:END:

In Backus-Naur form,
- all nonterminals names are delimited by
  angle brackets, ~⟨⟩~,
  - (if using ASCII characters, ~<>~)
- the ~⟶~ is replaced by ~∷=~,
- additional whitespace is permitted on the right side
  of a production between terminals and nonterminals,
  without changing the meaning of the production
  - So $⟨A⟩ ∷= a\ a\ ⟨A⟩$ is treated the same as $⟨A⟩ ∷= aa⟨A⟩$.

** Aside: ALGOL
:PROPERTIES:
:CUSTOM_ID: Aside:-ALGOL
:END:

ALGOL (for “ALGOrithmic Language”)
was a contemporary of Fortran, Lisp, and Cobol.
- Together, those three are the oldest languages
  still in (fairly) common use today.
  - Granted, not the same versions.

Specifically, there were several iterations of ALGOL,
the three major ones being ALGOL 58, ALGOL 60 and ALGOL 68.

ALGOL is not in common use, but it was
the most influential on modern programming language syntax,
introducing concepts such as the block.
- The “C family” can trace its lineage directly to ALGOL.

** Extended Backus-Naur form (EBNF)
:PROPERTIES:
:CUSTOM_ID: Extended-Backus-Naur-form-(EBNF)
:END:

We further extend our grammar notation to include several
several additional operators.
- These extensions are part of the /extended/ Backus-Naur form.
- Once again, this is only an extension in the /practicality/ sense.

There is an [[https://www.iso.org/standard/26153.html][ISO standard]] for EBNF.
Our syntax and inclusion of features is
not chosen to match the standard;
it is what is convenient for our use.

** EBNF details
:PROPERTIES:
:CUSTOM_ID: EBNF-details
:END:

- (Square) brackets, ~[]~, surrounding a string
  indicate that string may or may not be included in a production.
  - I.e., they make part of a production optional.
  - $⟨A⟩ ∷= α₁ [ α₂ ] α₃ \ \ \ ≈ \ \ \ ⟨A⟩ ∷= α₁ α₂ α₃ | α₁ α₃$.
- (Curly) braces, ~{}~, surrounding a string
  indicate that string may be repeated any number of times,
  including zero.
  - $⟨A⟩ ∷= α₁ { α₂ } α₃ \ \ \ ≈ \ \ \ ⟨A⟩ ∷= α₁ ⟨A′⟩ α₃$, $⟨A′⟩ ∷= α₂ ⟨A′⟩ | ε$.
- Parentheses, ~()~, may group parts of a string.
- The “alternative” pipe, ~|~, may be used /inside/ of productions,
  to indicate alternatives inside a set of brackets, braces
  or parentheses.
  - $⟨A⟩ ∷= α₁ (α₂ | α₃) α₄ \ \ \ ≈ \ \ \ ⟨A⟩ ∷= α₁ α₂ α₄ | α₁ α₃ α₄$.
- Where necessary, terminals may be single or double quoted,
  such as to indicate a whitespace character, pipe or quote.
  - $⟨\text{ebnfprods}⟩ ∷= ⟨\text{string}⟩ | ⟨\text{string}⟩ ⟨\text{optws}⟩ “|” ⟨\text{optws}⟩ ⟨\text{ebnfprods}⟩$

** Exercise – translating to EBNF
:PROPERTIES:
:CUSTOM_ID: Exercise-–-translating-to-EBNF
:END:

Translate this grammar from an earlier exercise to EBNF syntax.
#+begin_src text
A ⟶ B | C
B ⟶ aaB | ε
C ⟶ aaaC | ε
#+end_src
Then try to reduce the number of productions in the grammar,
while maintaining the language defined.

Can you use only one production when using EBNF?

** EBNF's syntactic sugar
:PROPERTIES:
:CUSTOM_ID: EBNF's-syntactic-sugar
:END:

EBNF gives us our first example of /syntactic sugar/;
syntax that does not add new features to a language,
only more convenient notation.
- As shown above, any grammar using the additional operators
  can be translated into one not using them.
  - But this likely requires more productions.
  - And certainly more characters/space on the page.
  
Syntactic sugar is a common feature of programming languages.
- Example: (imperative) languages often include various kinds of loops,
  where only one (or sometimes none!) is truly necessary.

When we discuss programming languages formally,
we will usually omit constructs which are syntactic sugar.
- If anything, we may note how to represent them
  in a “core” language which includes less constructs.

** Exercise – a small language C-like language
:PROPERTIES:
:CUSTOM_ID: Exercise-–-a-small-language-C-like-language
:END:

Consider the following context-free language.
#+begin_example text
⟨stmt⟩   ∷= ⟨assign⟩ | ⟨stmt⟩ "; " ⟨stmt⟩ | "while " ⟨expr⟩ " do " ⟨stmt⟩ | ⟨ws⟩ ⟨stmt⟩ ⟨ws⟩
⟨assign⟩ ∷= ⟨var⟩ ⟨ws⟩ " := " ⟨expr⟩
⟨expr⟩   ∷= ⟨var⟩ | ⟨const⟩ | ⟨expr⟩ ⟨op⟩ ⟨expr⟩ | ⟨ws⟩ ⟨expr⟩ ⟨ws⟩
⟨var⟩    ∷= ('x' | 'y' | 'z') {⟨var⟩}
⟨const⟩  ∷= (1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0) {⟨const⟩}
⟨op⟩     ∷= '+' | '-' | '*' | '/' | '<' | '>' | '='
⟨ws⟩     ∷= {' '} | {'\n'}
#+end_example

Provide some example programs in this language.

Can you precisely describe the language in English?

** Example – EBNF for C++
:PROPERTIES:
:CUSTOM_ID: Example-–-EBNF-for-C++
:END:

A good example of the practicality EBNF for specifying
the syntax of languages is this
[[http://www.externsoft.ch/download/cpp-iso.html][EBNF grammar for C++]]
(presented in tabular form, rather than lists of productions
as we use).

The grammar is much, much larger than anything we will write,
but it is still quite concise for describing
a real-world programming language.

* Parsing and executable code
:PROPERTIES:
:CUSTOM_ID: Parsing-and-executable-code
:END:

** Preamble                                    :ignore:

We will briefly summarise the parsing process,
beginning with some important terms.
- In this course, we are primarily interested in
  the beginning of this process, up to the
  construction of parse trees.

** Atomic syntactic units
:PROPERTIES:
:CUSTOM_ID: Atomic-syntactic-units
:END:

We have mentioned that both regular expressions and
context-free grammars are used in the description of
the syntax of programming languages.

However, our example programming language earlier
was described exclusively by a context-free grammar.
- Even the smallest syntactic units of the language,
  the /atomic/ syntactic units, have been described by the grammars.
  - For instance, we have used the production
    $⟨const⟩  ∷= (1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0) \{⟨const⟩\}$
    which describes numerical constants.

This is not done in practice.

** Lexemes and tokens
:PROPERTIES:
:CUSTOM_ID: Lexemes-and-tokens
:END:

In practice,
- regular expressions are instead used to describe the
  atomic syntactic units of languages.
  - For example,
    - keywords such as ~if~ and ~while~, constant values such as ~0~ or ~"abc"~,
      or names such as ~height~ or ~sqrt~.
  - Lexemes cannot be broken down into meaningful pieces.
- Grammars are then used to describe the possible arrangements
  of lexemes.
  - The terminals of the grammar are then names for sets of lexemes,
    called /tokens/, rather than elements of $Σ$.
  - For instance,
    - the token ~while~ for the set containing only the
      keyword ~while~,
    - or the token ~int_literal~ for the set $\{ 0, 1, -1, 2, … \}$,
    - or the token ~var~ for the set of valid variable names.

** Parsing
:PROPERTIES:
:CUSTOM_ID: Parsing
:END:

Parsing is the process of translating a program
from plaintext
to executable instructions
- whether this is done
  - ahead of time (compiling) or
  - when the program is to be run (interpreting),
  parsing is a necessary step before execution.
- A computer cannot run unparsed higher level language code.
  
** The zeroth step – preprocessing
:PROPERTIES:
:CUSTOM_ID: The-zeroth-step-–-preprocessing
:END:

Many programming languages support some form
of /preprocessing directives/ which are
to be carried out before the parsing process
properly begins.
- Commonly, “macros”, which often are simply
  textual substitutions to be carried out.
  - But they can be used for significantly more;
    in some instances, these directives
    form a programming language themselves.

** The first step – lexical analysis
:PROPERTIES:
:CUSTOM_ID: The-first-step-–-lexical-analysis
:END:

After preprocessing, if it is present, comes the
the conversion of the plaintext source code
into a sequence of /tokens/.
- This process may be
  called /lexical analysis/, /lexing/ or /tokenising/.
- The program to carry this process out may be
  called a /lexer/ or /tokeniser/.
- Lexical analysis discards whitespace, comments, and any other
  text which is irrelevant to the machine.

** The second step – parsing (syntactic analysis)
:PROPERTIES:
:CUSTOM_ID: The-second-step-–-parsing-(syntactic-analysis)
:END:

After converting from plaintext to a string of tokens, the next
step of parsing is to construct the parse tree.

This step is part of the parsing process,
but it is also usually called parsing.
- It may also be called /syntactic analysis/.

** The third step – (static) semantic analysis
:PROPERTIES:
:CUSTOM_ID: The-third-step-–-(static)-semantic-analysis
:END:

Once the parse tree is constructed,
rules about the form of programs
which cannot be (or cannot easily be)
described by a grammar are enforced
by /(static) semantic analysis/.

These rules include type checking and variable scope checking,
issues we will discuss later in the course.

This process produces the /symbol table/, which maps
each identifier to its relevant information,
such as
- where it is declared in the source and
- its type.

** The fourth step – intermediate code generation
:PROPERTIES:
:CUSTOM_ID: The-fourth-step-–-intermediate-code-generation
:END:

Most high-level languages are not translated directly to machine code;
instead, they are translated to some /intermediate code/,
which is closer to machine code than the high-level language.

For instance, languages on the JVM are translated
to Java bytecode during compilation/interpretation.

This intermediate code can then be translated
into machine code by later steps.

** Visualising the entire parsing process
:PROPERTIES:
:CUSTOM_ID: Visualising-the-entire-parsing-process
:END:

#+begin_src ditaa :file media/parsing-whole.png :exports results
+-----------+    /--------------\    +--------------------+
| {d}       |    |              |    | {d}                |
| Plaintext +----+ Preprocesser +--->+ Expanded plaintext +-\
| cDDF      |    | cFDD         |    | cDFD               | |
+-----+-----+    \--------------/    +--------------------+ |
                                                            |
             /----------------------------------------------/
             |
/------------+---------------\    +--------------------------+
|                            |    | {d}                      |
|     Lexical analysis       |    |    Sequence of tokens    |
| (constructed from regexps) +--->| (tagged with attributes) +-\
| cFDD                       |    | cDFD                     | |
\----------------------------/    +-----------+--------------+ |
                                                               |
             /-------------------------------------------------/
             |
/------------+---------------\    +----------------------+
|                            |    | {d}                  |
|          Parser            |    |      Parse tree      |
| (constructed from grammar) +--->|                      +-\
| cFDD                       |    | cDFD                 | |
\----------------------------/    +----------------------+ |
                                                           |
              /--------------------------------------------/
              |
/-------------+--------------\    +-------------------------+
|                            |    | {d}                     |
| (Static) semantic analysis |    | Attributed parse tree,  |
|                            +--->|     symbol table        +-\
| cFDD                       |    | cDFD                    | |
\----------------------------/    +-------------------------+ |
                                                              |
              /-----------------------------------------------/
              |
/-------------+---------------\    +-----------------------+
|                             |    | {d}                   |
| Intermediate code generator |    | Intermediate language |
|                             +--->+          code         +-\
|  cFDD                       |    | cDFD                  | |
\-----------------------------/    +-----------------------+ |
                                                             |
         /---------------------------------------------------/
         |
/--------+--------------\    +-----------------+
|                       |    | {d}             |
| Intermediate language |    | Executable code |
|     implemention      |--->|                 |
| cFDD                  |    | cDDF            |
\-----------------------/    +-----------------+
#+end_src

#+RESULTS:
[[file:media/parsing-whole.png]]

* Compilation, interpretation, and hybrid appraoches
:PROPERTIES:
:CUSTOM_ID: Compilation,-interpretation,-and-hybrid-appraoches
:END:

We have mentioned above during the discussion of parsing
the notions of compilation and interpretation.

Let us define those terms.

** Compilation
:PROPERTIES:
:CUSTOM_ID: Compilation
:END:

A /compiler/ translates the whole program
(and any libraries or other code resources needed)
ahead of running it.
- High upfront cost (time), for increased efficiency at runtime
- Not portable; machine code is machine dependent.
  
** Interpreters
:PROPERTIES:
:CUSTOM_ID: Interpreters
:END:

An /interpreter/ translates the program /as we are running it/.
- No upfront cost, but less efficient.
- Portable; can be run on any machine with an interpreter.
  - Alleviates some of the programmer's responsibility.
    - One user (or group) writes the interpreter /once/
      (per machine type);
      it can be used by any number of users for any number programs.
- Efficiency is improved by using *just-in-time compilation*.
  - Store the result of interpretation so it can be used again.
- Can achieve better error reporting.
  - Relationship between original and translated codes is known at runtime.
  - This relationship is discarded when compiling code.
    
** Hybrid methods
:PROPERTIES:
:CUSTOM_ID: Hybrid-methods
:END:

/Hybrid methods/ compile into a special intermediate language,
which is then interpreted into machine code when the program is run.
- This intermediate language is usually similar to assembly.
  - But targets a virtual machine, not actual hardware!
- Usually called /bytecode/.
- Greatly offsets efficiency cost of interpretation.
- More portable than compiled code; just need
  a bytecode interpreter for each target machine.

* Ambiguity
:PROPERTIES:
:CUSTOM_ID: Ambiguity
:END:

We have discussed parse trees as a representation
of programs used during the parsing process.

Parse trees are extremely helpful because they allow us
to discard irrelevant details about program text,
and focus on the form of programs.

However, there is one significant problem which can occur:
what if a program has *multiple* parse trees?

It is desirable to have a single parse tree for every program.
- We should not admit two syntactic interpretations for a program!

This can happen quite frequently, and we must discuss
methods of eliminating such /ambiguity/.

** An example of ambiguity
:PROPERTIES:
:CUSTOM_ID: An-example-of-ambiguity
:END:

For instance, the string ~aa~ has four valid parse trees
under the grammar
#+begin_src text
⟨A⟩ ∷= a ⟨A⟩ | ⟨A⟩ a | ε 
#+end_src

Exercise: find all four valid parse trees for ~aa~ with the above
grammar.

** Removing ambiguity
:PROPERTIES:
:CUSTOM_ID: Removing-ambiguity
:END:

Three tools for removing ambiguity are
- requiring parentheses,
- introducing precedence rules, and
- introducing associativity rules.

** Enforcing precedence with a grammar
:PROPERTIES:
:CUSTOM_ID: Enforcing-precedence-with-a-grammar
:END:

To enforce precedence using a grammar:
- Create a hierarchy of non-terminals.
- Higher-precedence operators are produced lower in the hierarchy.
- For instance,
  - An additive term can be a addition of multiplicative terms,
    which is an addition of literals, which can be the negation
    of a constant, variable or term.

** Example grammar which enforces precedence
:PROPERTIES:
:CUSTOM_ID: Example-grammar-which-enforces-precedence
:END:

:TODO:

** Enforcing associativity with a grammar
:PROPERTIES:
:CUSTOM_ID: Enforcing-associativity-with-a-grammar
:END:

To enforce associativity using a grammar:
- Left associative operators should be produced by left recursive
  non-terminals.
- And right associative operators by right recursive non-terminals.
- Operators of the same precedence must associate the same way!

** Example grammar which enforces associativity
:PROPERTIES:
:CUSTOM_ID: Example-grammar-which-enforces-associativity
:END:

:TODO:

** What about “associative” operations?
:PROPERTIES:
:CUSTOM_ID: What-about-“associative”-operations?
:END:

You know that in mathematics,
we often avoid parentheses by declaring operations
to be /left associative/ or /right associative/.
- For a left associative operator ~⊕~,
  ~a ⊕ b ⊕ c = (a ⊕ b) ⊕ c~.
  - Examples include subtraction.
- For a right associative operator ~⊕~,
  ~a ⊕ b ⊕ c = a ⊕ (b ⊕ c)~.
  - Examples include exponentiation.
- An /associative/ operator is a ~⊕~ for which
  ~a ⊕ b ⊕ c = (a ⊕ b) ⊕ c = a ⊕ (b ⊕ c)~.

Recall that addition is an associative operator.
- So the choice of whether addition in a language associates to
  the right or to the left may seem arbitrary.
- But numerical types in programming are not necessarily
  the same as numerical types in math!
- Addition of floating point numbers /is not associative/.
  - Consider a binary representation with two-digit coefficients.
  - 1.0₂ × 2⁰ + 1.0₂ × 2⁰ + 1.0₂ × 2² has a different value depending
    upon parenthesisation.
    - :TODO:
    
* Abstract and concrete syntax; ignoring ambiguity
:PROPERTIES:
:CUSTOM_ID: Abstract-and-concrete-syntax;-ignoring-ambiguity
:END:

“Simple”, ambiguous grammars do have a place in describing
programming language syntax.
- Such grammars describe the /abstract syntax/ of the language.
  - As opposed to /concrete syntax/.
- Consider programs as /trees/ generated by the grammar
  for the abstract syntax of the language.
  - Trees do not admit ambiguity!
  - Such trees more efficiently represent programs.
    - The shape of the tree expresses structure.
    - Other unnecessary details may be left out.

:TODO:

* The /semantics/ of formal languages
:PROPERTIES:
:CUSTOM_ID: The-/semantics/-of-formal-languages
:END:

** COMMENT Old notes
:PROPERTIES:
:CUSTOM_ID: COMMENT-Old-notes
:END:

Unlike with syntax, there is not one universally used tool
for describing programming language semantics.

In this course we will primarily consider /operational semantics/.
- A formal description of the meaning programs as
  a series of computation steps on an abstract machine.
  - The machine should be more abstract, and more easily understood,
    than assembly language.
  - But still “simpler” than the language.
  - Stack machines and state diagrams are good candidates.

Additional approaches include
- Denotational semantics.
  - The meaning of programs are /denoted/ by mathematical objects.
    - Such as partial functions.
  - Have to consider /limits/ and non-termination.
- Axiomatic semantics.
  - The meaning of a program is given by a precondition/postcondition
    calculus.
    - Such as ~wp~; the “weakest-precondition” calculus.
  - Very useful for specification.

*** The kernel language approach
:PROPERTIES:
:CUSTOM_ID: The-kernel-language-approach
:END:

The “kernel language” approach to semantics can be used
for languages with many features and constructs.
- Choose a small “kernel” set of features/constructs.
- Describe the remainder of the language in terms of that kernal language.
- The kernel language may be described using the formal approaches
  mentioned.
- /Concepts, Techniques, and Models of Computer Programming/
  takes this approach.

*** More to come...
:PROPERTIES:
:CUSTOM_ID: More-to-come...
:END:

We will return to the discussion of semantics later in the course.

** Preamble                                    :ignore:

The /semantics/ of a language assigns a meaning to each sentence.
- In order to define a semantics, we must
  have in mind a /semantic domain/;
  - a domain of meanings into which we map sentences.
- For instance, if we are defining a language
  of natural numbers /Nat/, we will map sentences into the set ~ℕ~.
- Or map elements of a languages of propositions into ~𝔹~.
- We may often provide several different definitions of
  a particular mapping, to emphasise different details.

We may also have several semantic domains for a given language.
- In the case of programming languages,
  several domains of meaning have been proposed and used;
  the three most well known are
  - computing devices, whether a real-world machine or an /abstract/ machine,
    - this is known as /operational semantics/
  - (mathematical) functions,
    - this is known as /denotational semantics/
  - precondition/postcondition pairs
    - this is known as /axiomatic semantics/

** Example – semantics of a language of natural numbers
:PROPERTIES:
:CUSTOM_ID: Example-–-semantics-of-a-language-of-natural-numbers
:END:

Consider again a language of terms intended to represent
natural numbers.
#+begin_src text
⟨nat⟩ ∷= "zero" | "suc" ⟨nat⟩ 
#+end_src

To assign meaning to these terms,
we introduce a mapping from these (concrete) terms
to (abstract) numerals.
#+begin_src text
eval zero = 0
eval (suc n) = (eval n) + 1
#+end_src

The evaluation function in this case is very obvious and trivial,
because with this language is simply a concrete representation
of the semantic domain.
- In comparison, when defining the semantics of programming languages,
  the language and the semantic domain are not so directly related.

** Example – semantics of propositional logic
:PROPERTIES:
:CUSTOM_ID: Example-–-semantics-of-propositional-logic
:END:

As a more complex example, we can map propositional logic terms
into the set of booleans.
#+begin_src text
⟨prop⟩ ∷= "tt" | "ff" | ¬ ⟨prop⟩ | ⟨prop⟩ (∧ | ∨ | ⇒ | ⇔) ⟨prop⟩
#+end_src

In order to make the mapping less trivial, let us define it
without using boolean combinators; only constants
and “if-then-else” statements.
#+begin_src text
eval tt = true
eval ff = false

eval (¬ p) = true    if eval p
             false   otherwise

eval (p ∧ q) = eval q   if eval p
               false    otherwise

…
#+end_src
Exercise: Complete this evaluation function.

** Example – small-step semantics of propositional logic
:PROPERTIES:
:CUSTOM_ID: Example-–-small-step-semantics-of-propositional-logic
:END:

The evaluation function defined above can be considered
to be a /big-step/ semantics.
- It is a (single-valued) relation between terms and
  their (final) value.

In contrast, we may define a /small-step/ semantics
- which maps terms to terms which are “one step” simpler.
- Then, once we have reduced to a constant term, that may be mapped
  to a value (this part is not shown here).
#+begin_src text
reduce (¬ tt) = ff
reduce (¬ ff) = tt
reduce (¬ p)  = ¬ (reduce p)

reduce (tt ∧ q) = reduce q
reduce (ff ∧ q) = ff
reduce (p ∧ q)  = (reduce p) ∧ q

…
#+end_src
Exercise: Complete this reduction function.
